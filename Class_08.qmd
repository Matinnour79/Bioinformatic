---
title: "Class 8: Mini-Project"
author: "Matin Nourmohammadi"
format: html
---

# 1. Exploratory data analysis

First, we need to read the file.

```{r}
fna.data <-  "WisconsinCancer.csv"
wisc.df <-  read.csv(fna.data, row.names=1)
wisc.df
head(wisc.df)
```

Next, we need to remove the first column out.

```{r}
wisc.data <- wisc.df[,-1]
wisc.data
```

Now, we need to save the `diagnosis` column into a separate vector.

```{r}
diagnosis <- factor(wisc.df$diagnosis) 
diagnosis
```

-   **Q1**. How many observations are in this dataset?

```{r}
nrow(wisc.df)
```

There are 569 observations.

-   **Q2**. How many of the observations have a malignant diagnosis?

```{r}
table(wisc.df$diagnosis)
```

There are 212 malignant diagnosis.

-   **Q3**. How many variables/features in the data are suffixed with `_mean`?

```{r}
sum(grepl("_mean$", names(wisc.df)))
```

There are 10 variables with the suffix `_mean`.

# 2. Principal Component Analysis

We need to use colMean and apply.

```{r}
colMeans(wisc.data)
```

```{r}
colMeans(wisc.data)
```

Now we can use PCA with `prcomp`:

```{r}
wisc.pr <- prcomp(scale(wisc.data))
wisc.pr
```

```{r}
summary(wisc.pr)
```

-   **Q4**. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

0.4427

-   **Q5**. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

PC1 + PC2= 0.4427+ 0.1897 = 0.6324

At least 2.

-   **Q6**. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

PC1 till PC7= 0.4427 + 0.1897 +0.0939 + 0.06605 + 0.05596 + 0.04044 + 0.0222 = 0.9117

Now we can generate graphs:

```{r}
biplot(wisc.pr)
```

-   Q7. What stands out to you about this plot? Is it easy or diﬀicult to understand? Why? The graph is not readable and it is a mess.

Let's generate more standard scatter plot for PC1 and PC2:

```{r}
plot( wisc.pr$x[,1:2] , col = diagnosis ,
        xlab = "PC1", ylab = "PC2")
```

-   **Q8.** Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

```{r}
 plot(wisc.pr$x[,c(1,3)], col = diagnosis,
       xlab = "PC1", ylab = "PC3")
 
```

Now we can use ggplot:

```{r}
df <- as.data.frame(wisc.pr$x)
  df$diagnosis <- diagnosis
  library(ggplot2)
  ggplot(df) +
    aes(PC1, PC2, col=diagnosis) +  geom_point()
```

Let's calculate the variance of each principal component by squaring the sdev component of wisc.pr:

```{r}
pr.var <- wisc.pr$sdev^2
  pr.var

```

```{r}
pve <- pr.var / sum(pr.var)
  pve
```

We can now graph the variance:

```{r}
plot(pve, xlab = "Principal Component",
       ylab = "Proportion of Variance Explained",
       ylim = c(0, 1), type = "o")
```

```{r}
barplot(pve, ylab = "Precent of Variance Explained",
       names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

-   **Q9.** For the first principal component, what is the component of the loading vector (i.e. `wisc.pr$rotation[,1]`) for the feature `concave.points_mean`? This tells us how much this original feature contributes to the first PC.

```{r}
 wisc.pr$rotation["concave.points_mean",1]
sort(wisc.pr$rotation[,1])
```

# 3. Hierarchical clustering

First, we need to scale the data:

```{r}
data.scaled <- scale(wisc.data)
   data.scaled
```

Now we can use `hclust`:

```{r}
wisc.hclust <- hclust(data.dist, method="complete")
  wisc.hclust

```
